{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aerial_Cactus_Identification_Step2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOZCbkAsHdgkcPz3/ej8UaH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohyunyang/My_Kaggle/blob/main/Aerial_Cactus_Identification_Step2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 베이스라인모델"
      ],
      "metadata": {
        "id": "ngVunML-8Dl4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이토치를 활용해 딥러닝 모델을 만들어보자\n",
        "\n",
        "베이스라인은 CNN 활용\n",
        "\n",
        "우선 시드값 고정 및 GPU 장비 설정을 해야한다"
      ],
      "metadata": {
        "id": "7ByMgjIV8FjV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 시드값 고정 \n",
        "\n",
        "시드값고정 : 단순히 결과 재현을 위한 작업\n",
        "\n",
        "GPU 장비 설정 : 훈련 속도를 높이기 위해 데이터를 GPU가 처리하도록 변경"
      ],
      "metadata": {
        "id": "sufG9r-o8JgM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 데이터 준비 \n",
        "\n",
        "훈련 / 검증 데이터 분리\n",
        "\n",
        "데이터셋 클래스 정의 : 이미지데이터를 모델링에 적합한 형태로 불러오도록 해줌\n",
        "\n",
        "데이터셋 생성 \n",
        "\n",
        "데이터 로더 ( 데이터셋으로 부터 데이터를 배치 단위로 불러와주는 객체) 생성"
      ],
      "metadata": {
        "id": "R7I4ZqeE8L32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 모델 생성\n",
        "\n",
        "신경망 모델 클래스를 직접 설계한 후 인스턴스 생성"
      ],
      "metadata": {
        "id": "hzF-46678NVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 모델 훈련\n",
        "\n",
        "손실함수와 옵티마이저 설정 : 훈련에 앞서 손실 함수와 *옵티마이저 설정\n",
        "\n",
        "모델훈련 : 신경망의 가중치(파라미터)를 갱신하며 모델 훈련\n",
        "\n",
        "성능 검증 : 검증데이터로 모델 성능 검증\n",
        "\n",
        "예측 및 제출 \n",
        "\n",
        "`*최적의 가중치를 찾아주는 알고리즘`"
      ],
      "metadata": {
        "id": "zYRwdALu8PwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 시드값 고정 및 GPU 장비 설정\n",
        "\n",
        "딥러닝과 파이토치 특성 때문에 추가된 단계"
      ],
      "metadata": {
        "id": "2aasaACI8TWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 시드값 고정 "
      ],
      "metadata": {
        "id": "ZgJCh_Pa8VDB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YndnFcaO61ju"
      },
      "outputs": [],
      "source": [
        "import torch # 파이토치 \n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 시드값 고정\n",
        "seed = 50\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "random.seed(seed)                # 파이썬 난수 생성기 시드 고정\n",
        "np.random.seed(seed)             # 넘파이 난수 생성기 시드 고정\n",
        "torch.manual_seed(seed)          # 파이토치 난수 생성기 시드 고정 (CPU 사용 시)\n",
        "torch.cuda.manual_seed(seed)     # 파이토치 난수 생성기 시드 고정 (GPU 사용 시)\n",
        "torch.cuda.manual_seed_all(seed) # 파이토치 난수 생성기 시드 고정 (멀티GPU 사용 시)\n",
        "torch.backends.cudnn.deterministic = True # 확정적 연산 사용\n",
        "torch.backends.cudnn.benchmark = False    # 벤치마크 기능 해제\n",
        "torch.backends.cudnn.enabled = False      # cudnn 사용 해제"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> GPU 장비 설정\n",
        "\n",
        "이어서 장비를 설정해야한다.\n",
        "\n",
        "정형데이터(csv파일 등)을 다루는 머신러닝과 달리 딥러닝은 주로 비정형데이터(이미지, 음성, 텍스트 등)을 다룬다.\n",
        "\n",
        "비정형 데이터를 모델링하려면 연산량이 많아진다. 해서 CPU로는 감당하기 벅찰 정도라 훈련시간이 너무 길어진다.\n",
        "\n",
        "그래서 GPU를 사용해야한다. GPU는 단순 연산 수백 수만개 이상을 병렬로 처리할 수 있어서 빠르게 훈련 가능하다."
      ],
      "metadata": {
        "id": "rQvUJvRa8dCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')"
      ],
      "metadata": {
        "id": "Na6sMbUd65Td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # 이렇게도 가능"
      ],
      "metadata": {
        "id": "6E9UbJ6GjD5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24wC6WcK67jY",
        "outputId": "78663449-2caa-4d97-ed41-79212f180750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "t43sTDJQ6-Ef",
        "outputId": "3308376a-fb0e-42d0-ba1e-4f15fffea9dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.6.15)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e80b355a-b5e2-4726-8678-674d3e01ef3a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e80b355a-b5e2-4726-8678-674d3e01ef3a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"yhyunmo\",\"key\":\"8fdb7bbf0193b24d0178986819fcf478\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 데이터준비"
      ],
      "metadata": {
        "id": "ZwZykeHO8iix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "! kaggle competitions download -c aerial-cactus-identification\n",
        "! ls\n",
        "! unzip aerial-cactus-identification.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VX8YD89v6-Kg",
        "outputId": "6d221304-68df-4ef0-8af7-03512fe58473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading aerial-cactus-identification.zip to /content\n",
            " 42% 5.00M/12.0M [00:00<00:00, 52.3MB/s]\n",
            "100% 12.0M/12.0M [00:00<00:00, 102MB/s] \n",
            "aerial-cactus-identification.zip  kaggle.json  sample_data\n",
            "Archive:  aerial-cactus-identification.zip\n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test.zip                \n",
            "  inflating: train.csv               \n",
            "  inflating: train.zip               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "labels = pd.read_csv('train.csv')\n",
        "submission = pd.read_csv('sample_submission.csv')"
      ],
      "metadata": {
        "id": "EBPsuvcl6-Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "# 훈련 이미지 데이터 압축 풀기\n",
        "with ZipFile('train.zip') as zipper:\n",
        "  zipper.extractall()\n",
        "\n",
        "# 테스트 이미지 데이터 압축 풀기\n",
        "with ZipFile('test.zip') as zipper:\n",
        "  zipper.extractall()"
      ],
      "metadata": {
        "id": "TRMTxWbA6-Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 훈련데이터 검증데이터 분리"
      ],
      "metadata": {
        "id": "QaQsLN4-8lpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 훈련 데이터, 검증 데이터 분리\n",
        "train, valid = train_test_split(labels, \n",
        "                                test_size=0.1,\n",
        "                                stratify=labels['has_cactus'],\n",
        "                                random_state=50)"
      ],
      "metadata": {
        "id": "1d1q1rwz6-cV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 데이터 개수 : ', len(train))\n",
        "print('검증 데이터 개수 : ', len(valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "friOUxiZ8oL1",
        "outputId": "9fd71fe0-52a0-4037-b2b8-80199e5a8199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터 개수 :  15750\n",
            "검증 데이터 개수 :  1750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 데이터셋 클래스 정의\n",
        "\n",
        "파이토치에서 제공하는 Dataset 클래스를 활용해 클래스 객체를 만들 수 있다\n",
        "\n",
        "Dataset은 추상 클래스이며 우리는 Dataset을 상속받은 다음 특수 메서드인 \"__len__\"()과 __getitem__() 을 재정의(오버라이딩) 해야 한다."
      ],
      "metadata": {
        "id": "yPe9Ozzx8rNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 # OpenCV 라이브러리\n",
        "from torch.utils.data import Dataset # 데이터 생성을 위한 클래스\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    # 초기화 메서드(생성자)\n",
        "    def __init__(self, df, img_dir='./', transform=None):\n",
        "        super().__init__() # 상속받은 Dataset의 생성자 호출\n",
        "        # 전달받은 인수들 저장\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "    \n",
        "    # 데이터셋 크기 반환 메서드 \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    # 인덱스(idx)에 해당하는 데이터 반환 메서드 \n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.df.iloc[idx, 0]    # 이미지 ID\n",
        "        img_path = self.img_dir + img_id # 이미지 파일 경로 \n",
        "        image = cv2.imread(img_path)     # 이미지 파일 읽기 \n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # 이미지 색상 보정\n",
        "        label = self.df.iloc[idx, 1]     # 이미지 레이블(타깃값)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image) # 변환기가 있다면 이미지 변환\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "_JalgRBS6-iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 데이터셋 생성"
      ],
      "metadata": {
        "id": "B61ITpdG8tG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms # 이미지 변환을 위한 모듈\n",
        "\n",
        "transform = transforms.ToTensor()"
      ],
      "metadata": {
        "id": "8RxDxzAJ6-of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "torchvision은 파이토치용 컴퓨터 비전 라이브러리다\n",
        "\n",
        "transforms 은 다양한 이미지 변환기를 제공하는 모듈이다\n",
        "\n",
        "ToTensor() 메서드로 이미지를 텐서로 바꿨다. 이때 (가로픽셀 수, 세로픽셀 수, 채널 수) 형상이 (채널 수, 가로필셀 수, 세로픽셀 수) 형상으로 바뀜\n",
        "\n",
        "파이토치르 이미지처리 할 때는 형상이 (채널 수, 가로필셀 수, 세로픽셀 수) 이여야 함\n",
        "\n",
        "32 x 32 x 3 => 3 x 32 x 32 로 바뀐다는 뜻\n",
        "\n",
        "여기에 배치가 추가되면 (배치크기, 채널 수, 가로필셀 수, 세로픽셀 수)"
      ],
      "metadata": {
        "id": "oPjGc1uq8vii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = ImageDataset(df=train, img_dir='train/', transform=transform)\n",
        "dataset_valid = ImageDataset(df=valid, img_dir='train/', transform=transform)"
      ],
      "metadata": {
        "id": "qDtfI8YU6-ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 데이터 로더 생성\n",
        "\n",
        "데이터셋 다음으로는 데이터로더를 생성해야한다\n",
        "\n",
        "데이터로더는 지정한 배치크기만큼 데이터를 불러오는 객체이다\n",
        "\n",
        "딥러닝모델을 훈련할 때는 주로 단위로 데이터를 가져와 훈련한다\n",
        "\n",
        "묶음단위로 훈련하는 게 훨씬 빠르게 때문이다"
      ],
      "metadata": {
        "id": "Gc8LOFoL8yeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader # 데이터 로더 클래스\n",
        "\n",
        "loader_train = DataLoader(dataset=dataset_train, batch_size=32, shuffle=True)\n",
        "loader_valid = DataLoader(dataset=dataset_valid, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "-oEYUmd16-07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- dataset : 앞서 만든 이미지 데이터셋을 전달한다\n",
        "- batch_size : 배치크기다. 훈련데이터와 검증 데이터 모두 한 번에 32개씩 불러오게 설정함 ( 4~ 256 까지 다양함, 그외 값도 가능, 배치크게에 정답은 없다)\n",
        "- shuffle : 데이터를 섞을지 여부를 결정한다. 특정 데이터가 몰려 있을 경우를 대비, 훈련데이터에는 True 를 전달해 데이터를 섞었다"
      ],
      "metadata": {
        "id": "6_SR1TOJ803v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 모델생성"
      ],
      "metadata": {
        "id": "qMN9cmw981-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이번 절에는 기본적인 합성곱 신경망 CNN 모델을 만들어보겠다\n",
        "(3, 32, 32) 형상의 이미지 데이터를 두 번의 합성곱과 풀링, 평탄화, 전결합 등을 거쳐 최종적으로 값이 0일 확률과 1일 확률을 구할 것이다"
      ],
      "metadata": {
        "id": "rC97uSUY84wA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(3,32,32) ▶ 합성곱 ▶ 최대풀링 ▶ 합성곱 ▶ 최대풀링 ▶ 평균풀링 ▶ 평탄화 ▶ 전결합 ▶ 출력(0일확률/1일확률)"
      ],
      "metadata": {
        "id": "6ftrWDZy85Xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn # 신경망 모듈\n",
        "import torch.nn.functional as F # 신경망 모듈에서 자주 사용되는 함수\n",
        "\n",
        "class Model(nn.Module):\n",
        "    # 신경망 계층 정의 \n",
        "    def __init__(self):\n",
        "        super().__init__() # 상속받은 nn.Module의 __init__() 메서드 호출\n",
        "        \n",
        "        # 첫 번째 합성곱 계층 \n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, \n",
        "                               kernel_size=3, padding=2) \n",
        "        # 두 번째 합성곱 계층 \n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, \n",
        "                               kernel_size=3, padding=2) \n",
        "        # 최대 풀링 계층 \n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=2) \n",
        "        # 평균 풀링 계층 \n",
        "        self.avg_pool = nn.AvgPool2d(kernel_size=2) \n",
        "        # 전결합 계층 \n",
        "        self.fc = nn.Linear(in_features=64 * 4 * 4, out_features=2)\n",
        "        \n",
        "    # 순전파 출력 정의 \n",
        "    def forward(self, x):\n",
        "        x = self.max_pool(F.relu(self.conv1(x)))\n",
        "        x = self.max_pool(F.relu(self.conv2(x)))\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(-1, 64 * 4 * 4) # 평탄화\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "hD2fxoBW7iTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Model 클래스를 정의하는 또 다른 방법"
      ],
      "metadata": {
        "id": "7M_Zwsh6893A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__() \n",
        "        # 첫 번째 합성곱, 최대 풀링 계층\n",
        "        self.layer1 = nn.Sequential(nn.Conv2d(in_channels=3, \n",
        "                                              out_channels=32, \n",
        "                                              kernel_size=3, \n",
        "                                              padding=2),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.MaxPool2d(kernel_size=2))\n",
        "        # 두 번째 합성곱, 최대 풀링 계층\n",
        "        self.layer2 = nn.Sequential(nn.Conv2d(in_channels=32, \n",
        "                                              out_channels=64, \n",
        "                                              kernel_size=3, \n",
        "                                              padding=2),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.MaxPool2d(kernel_size=2))\n",
        "        # 평균 풀링 계층\n",
        "        self.avg_pool = nn.AvgPool2d(kernel_size=2) \n",
        "        # 전결합 계층\n",
        "        self.fc = nn.Linear(in_features=64 * 4 * 4, out_features=2)\n",
        "        \n",
        "    # 순전파 출력 정의\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(-1, 64 * 4 * 4) # 평탄화\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "4tESFsHU9B7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model().to(device)\n",
        "\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCgTRG-_7i7N",
        "outputId": "a284905f-e6c0-4864-a521-fba55bbcadb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "  (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (avg_pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델훈련"
      ],
      "metadata": {
        "id": "hZu8tK3B9EDr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 손실함수 설정\n",
        "\n",
        "신경망 모델훈련은 가중치를 갱신하는 작업이다 \n",
        "\n",
        "예측값과 실제값의 손실이 작아지는 방향으로 이루어진다\n",
        "\n",
        "손실 함수로 교차엔트로피를 사용하겠다"
      ],
      "metadata": {
        "id": "cZkwCb8q9E7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실함수\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "oEyRPdks7j00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 옵티마이저 설정\n",
        "\n",
        "첫 번째 파라미터로 모델이 가진 파라미터들을 전달한다. (model.parameters())\n"
      ],
      "metadata": {
        "id": "4Dcba9Ip9HPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 옵티마이저\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "tJD6C1077p_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 훈련\n",
        "\n",
        "- 1. 데이터 로더에서 배치 크기만큼 데이터를 불러온다\n",
        "- 2. 불러온 이미지 데이터와 레이블(타깃값) 데이터를 장비(GPU 혹은 CPU)에 할당한다\n",
        "- 3. 옵티마이저 내 기울기를 초기화한다\n",
        "- 4. 신경망 모델에 입력데이터(이미지)를 전달해 순전파하여 출력값(예측값)을 구한다\n",
        "- 5. 예측값과 실제 레이블(타깃값)을 비교해 손실을 계산한다.\n",
        "- 6. 손실을 기반으로 역전파를 수행한다\n",
        "- 7. 역전파로 구한 기울기를 활용해 가중치를 갱신한다\n",
        "- 8. 1~7 절차를 반복횟수만큼 되풀이한다\n",
        "- 9. 1~8 절파를 에폭만큼 반복한다"
      ],
      "metadata": {
        "id": "BCACbSLg9KUI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련데이터는 15,570개, 배치크기는 32로 설정했으니 \n",
        "\n",
        "15,750 / 32 를 올림한 493번이다"
      ],
      "metadata": {
        "id": "VoSXTve29NDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "math.ceil(len(train) / 32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5iBuW_y7qFV",
        "outputId": "b45f97a8-b1c3-4e43-9a86-05493c9e4cd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "493"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(loader_train) # 데이터로더는 하나의 배치를 한 묶음으로 처리하기 때문"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lipua37c9OSE",
        "outputId": "d6289b4b-3cd8-4240-bd82-e9ff7c3c7796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "493"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 모델훈련"
      ],
      "metadata": {
        "id": "JGXdRTkk9PaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10 # 총 에폭\n",
        "# 총 에폭만큼 반복\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0 # 에폭별 손실값 초기화\n",
        "    \n",
        "    # '반복 횟수'만큼 반복 \n",
        "    for images, labels in loader_train: # 데이터로더의 길이 493번 만큼 반복 /loader_train로 부터 배치크기 32만큼의 이미지와 레이블추출해 변수할당\n",
        "        # 이미지, 레이블 데이터 미니배치를 장비에 할당 \n",
        "        images = images.to(device) # 훈련시 GPU 또는 CPU 활용하도록 함\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # 옵티마이저 내 기울기 초기화\n",
        "        optimizer.zero_grad()  # 가 없다면 기존 기울기가 계속 누적됨 해서 초기화하는 이처럼 작업필요\n",
        "        # 순전파 : 이미지 데이터를 신경망 모델의 입력값으로 사용해 출력값 계산\n",
        "        outputs = model(images)\n",
        "        # 손실 함수를 활용해 outputs와 labels의 손실값 계산\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 현재 배치에서의 손실 추가\n",
        "        epoch_loss += loss.item() \n",
        "        # 역전파 수행\n",
        "        loss.backward()\n",
        "        # 가중치 갱신\n",
        "        optimizer.step() # 새로운 가중치 = 기존가중치 - (학습률 x 기울기)\n",
        "        \n",
        "    # 훈련 데이터 손실값 출력\n",
        "    print(f'에폭 [{epoch+1}/{epochs}] - 손실값: {epoch_loss/len(loader_train):.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nmM_JPU7qLT",
        "outputId": "4ece8846-8e40-4f30-be8e-2311e604c753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에폭 [1/10] - 손실값: 0.1190\n",
            "에폭 [2/10] - 손실값: 0.1188\n",
            "에폭 [3/10] - 손실값: 0.1171\n",
            "에폭 [4/10] - 손실값: 0.1118\n",
            "에폭 [5/10] - 손실값: 0.1097\n",
            "에폭 [6/10] - 손실값: 0.1050\n",
            "에폭 [7/10] - 손실값: 0.1065\n",
            "에폭 [8/10] - 손실값: 0.1022\n",
            "에폭 [9/10] - 손실값: 0.1005\n",
            "에폭 [10/10] - 손실값: 0.0984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 성능검증"
      ],
      "metadata": {
        "id": "WlU1MNrU-9zP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련이 끝났으니 검증데이터를 이용해 평가지표인 ROC AUC 값을 구해보자"
      ],
      "metadata": {
        "id": "qV8Rei2j7qP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score # ROC AUC 점수 계산 함수 임포드\n",
        "# 실제값과 예측확률값을 담을 리스트 초기화\n",
        "true_list = [] # 실제값을 담을 리스트\n",
        "preds_list = [] # 예측값을 담을 리스트"
      ],
      "metadata": {
        "id": "lG8gwloH7qUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 모델 성능검증"
      ],
      "metadata": {
        "id": "ajHQVacW_VMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval() # 모델을 평가상태로 설정\n",
        "\n",
        "with torch.no_grad(): # 기울기 계산 비활성화(왜?)\n",
        "  for images, labels in loader_valid: \n",
        "    # 이미지, 레이블데이터 미니배치를 장비에 할당\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # 순전파 : 이미지데이터를 신경망 모델의 입력값으로 사용해 출력값 계산\n",
        "    outputs = model(images)\n",
        "    preds = torch.softmax(outputs.cpu(), dim=1)[:, 1] # 예측확률\n",
        "    true = labels.cpu() # 실제값\n",
        "    # 예측확률과 실제값을 리스트에 추가\n",
        "    preds_list.extend(preds)\n",
        "    true_list.extend(true)\n",
        "\n",
        "# 검증데이터 ROC AUC 점수 계산\n",
        "print(f'검증데이터 ROC AUC:, {roc_auc_score(true_list, preds_list)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0L1ofTU_bpi",
        "outputId": "67702051-d637-4d27-c21b-f474dd532a1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "검증데이터 ROC AUC:, 0.9930564283021238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(preds_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_X3drMbAr3H",
        "outputId": "dd3d808a-7f77-4fde-ff71-926df081ee62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5250"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(true_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTQniWp5AwCy",
        "outputId": "848c1f92-6f86-4a4a-ad63-2690ee6d331b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5250"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`preds = torch.softmax(outputs.cpu(), dim=1)[:, 1]`\n",
        "\n",
        "ROC AUC 를 구하려면 각 타깃값의 확률을 먼저 구해야한다\n",
        "\n",
        "그래서 순전파 출력값인 outputs를 torch.softmax()에 넘겨 0일 확률과 1일 확률을 얻었다 "
      ],
      "metadata": {
        "id": "Pny2ds0iBKzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`outputs.cpu()` , `labels.cpu()`\n",
        "\n",
        "위 함수는 각각 이전에 GPU에 할당했던 outputs과 labels의 데이터를 다시 CPU에 할당한다\n",
        "\n",
        "그래야 roc_auc_socre() 함수로 ROC AUC값을 계산 할 수 있다\n",
        "\n",
        "roc_auc_socre() 는 파이토치가 아니라 사이킷런 함수이므로 GPU에 있는 데이터를 직접사용하는데서 이유가 있다"
      ],
      "metadata": {
        "id": "pF8DPWClBMqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 예측 및 결과 제출"
      ],
      "metadata": {
        "id": "LDl6UjcxBnVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test = ImageDataset(df=submission, img_dir='test/', transform= transform)\n",
        "loader_test = DataLoader(dataset= dataset_test, batch_size= 32, shuffle=False)"
      ],
      "metadata": {
        "id": "GEfjnmmICm8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터에서 타깃확률이 1일 확률을 예측해보겠다\n",
        "\n",
        "for문이 끝나면 preds 변수에 최종예측확률이 모두 저장되어 있을 것"
      ],
      "metadata": {
        "id": "oAnq4GL4C_YJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval() # 모델을 평가상태로 설정\n",
        "\n",
        "preds = [] # 타깃예측값 저장용 리스트 초기화\n",
        "\n",
        "with torch.no_grad(): # 기울기 계산 비활성화\n",
        "  for images, _ in loader_test:\n",
        "    # 이미지데이터 미니배치를 장비에 할당\n",
        "    images = images.to(device)\n",
        "\n",
        "    # 순전파 : 이미지 데이터를 신경망 모델의 입력값으로 사용해 출력값 계산\n",
        "    outputs = model(images)\n",
        "    # 타깃값이 1일 확률(예측값)\n",
        "    preds_part = torch.softmax(outputs.cpu(), dim=1)[:, 1].tolist() # tolist() 를 하지않으면 제출불가 / 이는 텐서타입을 리스트로 바꾼 것\n",
        "    # preds에 preds_part 이어붙이기 \n",
        "    preds.extend(preds_part)"
      ],
      "metadata": {
        "id": "g3EUbq25DHaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 결과 제출"
      ],
      "metadata": {
        "id": "SExxBhx5EA8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission['has_cactus'] = preds\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "K1ESSauvEB7j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}